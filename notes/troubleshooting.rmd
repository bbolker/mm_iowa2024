---
title: "mixed model troubleshooting"
---

# overview

## obligatory Tolstoy quote

* "all happy families are alike ..."
* ... but there are certainly *categories* of problems

## categories of warnings

* messages vs warnings vs errors
* convergence (numerical optimization questionable)
* singular fits (model is degenerate)
   * other forms of degeneracy (complete separation etc.)
* model diagnostics issues (model is inappropriate)

## messages vs warnings vs errors

* message = informational
* warning = potentially serious problem. Don't ignore unless you understand it, and if so, then eliminate the warning as far upstream as possible (e.g. by changing options/settings; `suppressWarnings()` if that's the only choice)
* error = so serious that R/package author can't or won't give you an answer (may be possible to override/work around ...)

# convergence problems

## non-positive-definite Hessians

* what does this mean?
* modern mixed-model software does some form of non-linear optimization
    * sometimes just the covariance parameters, sometimes fixed + covariance parameters
	* (random-effects parameters are profiled out or estimated in an internal loop)
* at the optimum we should have a gradient of zero and a **positive definite** Hessian
* Hessian:

\newcommand{\jjdiag}[1]{\frac{\partial^2 L}{\partial \beta_{#1}^2}}
\newcommand{\jjoff}[2]{\frac{\partial^2 L}{\partial \beta_{#1} \partial \beta_{#2}}}

\[
\left(
\begin{array}{cccc}
\jjdiag{1} & \jjoff{1}{2} & \jjoff{1}{3} & \ldots \\
\jjoff{2}{1} & \jjdiag{2} & \jjoff{2}{3} & \ldots \\
\jjoff{3}{1} & \jjoff{3}{2} & \jjdiag{3} & \ldots \\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)
\]


## 

* eigenvalues of the Hessian give curvature in the principal directions (== eigenvectors)

```{r echo = FALSE}
## add picture: curve3d + genrose
cc <- emdbook::curve3d(x^2+y^2+3*x*y, from = c(-1, -1), to = c(1,1),
                       sys3d = "contour", ann = FALSE, axes = FALSE,
                       xlab = "", ylab = "",
                       labels = "")
arrows(-0.25, -0.25, 0.25, 0.25, code = 3)
arrows(-0.25, 0.25, 0.25, -0.25, code = 3)
box()
```

## solutions

* double-check the model definition and data
* scale and center predictor variables
* extract Hessian and see *which* eigenvector(s) is/are associated with the minimal eigenvalue(s)
   * extracting Hessian often requires some digging, e.g. `model@optinfo$derivs` (`lme4`) or `model$sdr$cov.fixed` (`glmmTMB`))
   
**example**

## more examples

* many GLMM parameters have a *bounded domain*
   * variances/standard deviations/dispersion parameters (0 to $\infty$)
   * probabilities (0 to 1)
* the easiest way to handle bounded optimization is to fit a *transformed*, unconstrained version of the parameter
   * log (variance, SD, ratios thereof, odds ratios)
   * logit/log-odds (probabilities)
* problems when the estimate is actually on the boundary of the original domain (transformed parameter $\to \pm \infty$)
* different packages handle this in different ways

## example: negative binomial dispersion

* NBinom can only handle *overdispersion*, not equi- or underdispersion
* example:

```r
set.seed(102)
dd <- data.frame(x = rpois(1000, lambda = 2))
m1 <- MASS::glm.nb(x ~ 1, data = dd)
environment(m1$family$variance)$.Theta
```
* **solutions**
   * try to ignore it
   * revert to Poisson model
   * use a *quasi-likelihood* model (fit Poisson, then adjust SEs etc.)
   * use a distribution that allows underdispersion: (glmmTMB) `genpois`, `compois`, or an *ordinal* model

## example: non-zero-inflated models

* *zero-inflation* models fit a finite mixture of "structural" and "sampling" zeros
* "many zeros does not mean zero inflation" [@warton_many_2005]
* if 



## `lme4` convergence warnings

## a specific (problematic!) case

* extra check on top of optimizer-based convergence checks
* 
## diagnostics

* `allFit`
* are parameters close enough?
* what is the distribution of negative log-likelihoods?
* which parameters are unstable?

## singular fits

* 

```{r}
library(lme4)
data(Orthodont,package="nlme")
Orthodont$nsex <- as.numeric(Orthodont$Sex=="Male")
Orthodont$nsexage <- with(Orthodont, nsex*age)
m1 <- lmer(distance ~ age + (age|Subject) + (0+nsex|Subject),
           start = c(1.8,-0.1,0.14, 0.5),
           ##(0 + nsexage|Subject),
           data=Orthodont)
VarCorr(m1)
rePCA(m1)
library(nlme)
m2 <- lme(distance ~ age, random = ~age|Subject,
    data = Orthodont,
    weights = varIdent(form = ~1|Sex))
library(glmmTMB)
m3 <- glmmTMB(distance ~ age + (age|Subject),
              dispformula = ~Sex,
              REML = TRUE,
              data = Orthodont)
exp(fixef(m3)$disp)
logLik(m3)
```

## model reduction

* drop terms
* drop correlations
* reduced-rank models

## regularization

* `blme`
* other possibilities?

## complete separation (GLM(M)-specific)

* add priors

## other boundary-type problems (extended GLM(M)-specific)

* zero-inflation
* dispersion parameters (e.g. negative binomial)

