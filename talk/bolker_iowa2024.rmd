---
title: "Progress and challenges in open-source multilevel modeling"
author: Ben Bolker
date: 14 May 2024
bibliography: ../glmm.bib
csl: ../reflist2.csl
output:
  ioslides_presentation
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymbol}
- \newcommand{\y}{\boldsymbol y}
- \newcommand{\X}{\boldsymbol X}
- \newcommand{\Z}{\boldsymbol Z}
- \newcommand{\bbeta}{\boldsymbol \beta}
- \newcommand{\bb}{\boldsymbol b}
- \newcommand{\bu}{\boldsymbol u}
- \newcommand{\bLambda}{\boldsymbol \Lambda}
- \newcommand{\bEta}{\boldsymbol \eta}
- \newcommand{\btheta}{\boldsymbol \theta}
- \newcommand{\bzero}{\boldsymbol 0}
---

<style>
.refs {
   font-size: 14px;
}
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
.title-slide {
   background-color: #55bbff;
}

<!-- https://stackoverflow.com/questions/50378349/force-column-break-in-rmarkdown-ioslides-columns-2-layout -->
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, dpi = 200, optipng = knitr::hook_optipng)
options(bitmapType = "cairo")
```

```{r pkgs, message = FALSE}
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=16))
```

# Definitions and scope

## GLMMs: the biggest picture

* what is the scope of this talk?
* linear fixed effects (possibly with a link function)
* linear random effects/*latent variables* (ditto)
* known clusters with independent sets of latent variables
* multivariate Normal Gaussian distribution of latent variables

## in other words

$$
\begin{split}
\underbrace{Y_i}_{\text{response}} & \sim \overbrace{\text{Distr}}^{\substack{\text{conditional} \\ \text{distribution}}}(\underbrace{g^{-1}(\eta_i)}_{\substack{\text{inverse} \\ \text{link} \\ \text{function}}},\underbrace{\phi}_{\substack{\text{scale} \\ \text{parameter}}}) \\
\underbrace{\boldsymbol \eta}_{\substack{\text{linear} \\ \text{predictor}}} & 
 = 
\underbrace{\X \bbeta}_{\substack{\text{fixed} \\ \text{effects}}} + 
\underbrace{\Z \bb}_{\substack{\text{random} \\ \text{effects}}}
\\
\underbrace{\bb}_{\substack{\text{conditional} \\ \text{modes}}}  & 
\sim \text{MVN}(\boldsymbol 0, \underbrace{\Sigma(\boldsymbol \theta)}_{\substack{\text{covariance} \\ \text{matrix}}})
\end{split}
$$


... possibly allow **zero-inflation**/hurdle component, **dispersion** (scale) model (e.g. $\phi = \exp(\X_d \beta_d (? + \Z_d \bb_d ?))$)

# Past

## What is a "routine" model/analysis?

* computation time/cost
* availability of reliable software
* data size
* statistical complexity/sophistication
* ANOVA → linear models → GLMs → mixed models → ???

## Commoditization

* "goods ... that are distinguishable in terms of attributes (uniqueness or brand) end up becoming simple commodities in the eyes of the market or consumers" ([Wikipedia](https://en.wikipedia.org/wiki/Commoditization))
* basic components become part of a **stack**/utilities for higher-level features
* operating system > linear algebra packages (BLAS, LAPACK) > regression tools > generalized linear models ...
* build on top of old, well-tested frameworks

## Democratization

* as long as people are educated *enough*
* ¿can we afford enough statistical consultants?
* Efron: "recommending that scientists use [X] is like giving the neighborhood kids the key to your F-16" [@gelmanObjections2008]  
(*X = "Bayes' Theorem" here but could be "mixed models"?*)
* want to make easy things easy and hard things possible

## Domain-specific languages

* DSLs can express a *limited* set of requirements
* R!
* *Wilkinson-Rogers-Bates* formulas [@WilkinsonRogers1973]
    * initial target: designed experiments
	* `GENOTYPE*POL(SITE, 1,SITEMEAN)*POLND(DENSITY)`
	* GENSTAT, expanded in S/R [@chambersStatistical1992a]
	* expanded further to include random effects [@pinheiro_mixed-effects_2000; @tanakaSymbolic2019]
    * further expansions ... (`brms` package etc.)
* **too** compact [@mcelreath_statistical_2015]?  
who really knows what `y ~ x1*x2 + (x1*x2|g)` means?
	
# Present

## Ever-expanding possibilities

```{r echo=FALSE}
pkgs <- ctv:::.get_pkgs_from_ctv_or_repos("MixedModels")
```

* [Mixed models task view](https://cran.r-project.org/web/views/MixedModels.html): currently mentions `r length(pkgs[[1]])` packages ...

```{r nbr_graph, out.height=400, out.width = 400, fig.align = "center"}
knitr::include_graphics("nbr_graph.png")
```

## Development and maintenance

Open source software is:

* powerful
* equitable
* reproducible
* transparent
* cutting-edge
* cheap!

## Development and maintenance

<!-- https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html -->

:::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 50px;"}

::: {}

**BUT**:

* academics are paid to publish Cool Stuff
* researchers are paid to Get Stuff Done
* computational statisticians are rare
* mixed models are a niche market  
(`lme4` is approximately 92d (out of 21K) on CRAN)

:::

::: {}

```{r dep, out.width=300, out.height=300}
knitr::include_graphics("dependency_2x.png")
```

:::

::::

## Development models

:::: {style="display: flex;"}

Something here 

::: {}

:::

::: {}

```{r devel, out.width=300, out.height=300}
include_graphics("cathedral_bazaar.png")
```

:::

::::

## Consolidation

* `tidymodels` example
* `easystats`
* `broom.mixed`, `DHARMa`, `marginaleffects`


# Future

## Machine learning/AI

```{r ML}
include_graphics("ml_cartoon.png")
```

from [sandserifcomics](https://www.instagram.com/sandserifcomics/)

## ML or 'statistical' learning

ML/SL:

* emphasis on algorithms and scalability
* prediction over inference [@breimanStatistical2001]
* automatic differentiation

## autodiff everywhere

* magic algorithm
* CppAD, TMB, Stan, PyTorch, Tensorflow (`greta`), NIMBLE, ...

## prediction 'vs' inference

* any 'parameter' of a model is an expectation of a change in the response
   * partial dependence (*marginal effects*), conditional dependence: 
* users are almost always interested in *counterfactuals*
* ... and estimates of uncertainty (or they should be)
* uncertainty in a counterfactual effect == inference ???
* connections to **causal inference**

## Smooths everywhere

* Revisit this layer: $\bb \sim \text{MVN}(\boldsymbol 0, \Sigma(\boldsymbol \theta))$
* if $\Sigma$ is diagonal then this is equivalent to putting a *ridge penalty* ($\bb \bb^\top = \sum b_i^2$) on the conditional modes
* we could use structured covariance matrices (compound symmetry, AR1, etc.)
* ... penalizing by $\bb \Sigma \bb^T$ is equivalent to using a **smoothing matrix** $\Sigma^{-1}$
* Gaussian processes, splines, Markov random fields ...
* `mgcv` package provides implementations that can be used anywhere [@wood_generalized_2017]
    * including efficient low-rank approximations
    * `gamm4`, `glmmTMB`, `sdmTMB`, `brms` ...

## Scalability

* dimensions of a mixed model
   * number of observations ($10^2$ - $10^8$)
   * number of latent variables ($10$ - $10^4$)
   * number of clusters ($10$ - $10^4$)
   * number of top-level parameters ($\beta$, $\theta$): $10$ - $1000$
* MLE scaling typically ${\cal O}(N^{3/2})$; method-of-moments ${\cal O}(N)$  
(stochastic gradient descent???)
*  Recent work on scalable MM, [@heilingEfficient2024; @heilingglmmPen2024; @gaoEfficient2017; @gaoEstimation2020a; @ghoshScalable2022; @ghoshBackfitting2022; @ghandwaniScalable2023]

## Algorithms


## Sparsity

* computational trick or statement about the world?
* combining mixed models with sparsity-inducing penalization (e.g. `glmmLasso`)
* statement about an **effect size spectrum**
   * ridge penalty ($\sum \beta_i^2$) == Gaussian prior on effects
   * lasso penalty ($\sum |\beta_i|$) == Laplacian prior on effects
* or, what fits best??

## Sliding toward Bayes

* regularization/priors
   * Bayes: justified by *belief*
   * frequentism: justified by 
       * decreased bias (e.g. Firth logistic regression)
	   * decreased error ([Stein's paradox](https://en.wikipedia.org/wiki/Stein%27s_example), ridge regression)
* empirical Bayes: regularization/priors only on intermediate layers
* maximum *a posteriori* (MAP) estimation: INLA, `blme`
* for good confidence intervals, MCMC is faster than parametric bootstrapping (e.g. pharmacokinetic models)

## Why aren't we all Bayesian?

* concern about subjectivity, ease of use [@efronWhy1986]
* Bayes is still slower

```{r timings, fig.width=3, fig.height = 2}
theme_set(theme_bw(base_size=12))
timings <- readRDS("timecomp.rds") |> sort() |> rev()
dd <- data.frame(package = factor(names(timings), levels = names(timings)),
   time = timings)
ggplot(dd, aes(x = time, y = package)) + geom_point(size=4) +
  scale_x_log10() + labs(x="time (sec)", y = "")
```

## references {.refs .columns-2 .allowframebreaks}

